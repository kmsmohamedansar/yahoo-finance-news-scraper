# -*- coding: utf-8 -*-
"""Scraping yahoo finance.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krqeQ5gLN4JISQM9Lazd2c5_yhEcuABY
"""

# Step 1: Install Selenium and ChromeDriver for Colab.
# Selenium controls a real browser so we can get JavaScript-rendered content.

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin

# Step 2: Import Selenium and set up Chrome options for Colab headless browsing.

from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument("--headless") # Run in headless mode (no UI)
chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("--no-sandbox")

# Correct way: do NOT pass 'chromedriver' as the first argument!
driver = webdriver.Chrome(options=chrome_options)

# Step 3: Use Selenium to open the Yahoo Finance homepage.
# We'll wait a few seconds to ensure all JavaScript-rendered news content loads.

url = "https://finance.yahoo.com/"
driver.get(url)

import time
time.sleep(5)  # Wait for 5 seconds to let the page fully render

# Step 4: Get the fully rendered HTML from Selenium and parse it with BeautifulSoup.
# This allows us to search for news headlines in the loaded page.

from bs4 import BeautifulSoup
soup = BeautifulSoup(driver.page_source, "html.parser")

# Step 5: Find all news headlines and their links.
# We'll look for <h3> tags, which usually contain the news headlines on Yahoo Finance.
# Each <h3> should have an <a> child with the headline text and link.

news_links = []
for h3 in soup.find_all("h3"):
    a = h3.find("a")
    if a and a.text.strip():
        headline = a.text.strip()
        link = a['href']
        # Convert relative URLs to absolute
        if link.startswith('/'):
            link = "https://finance.yahoo.com" + link
        news_links.append({"headline": headline, "url": link})

print(f"Found {len(news_links)} news articles on the homepage.")
news_links[:5]  # Show a sample

# Step 6: For each news article, use Selenium to visit the link and extract details.
# We'll collect the headline, date, author, and full article content.

detailed_articles = []

for i, article in enumerate(news_links[:5]):  # Limit to first 5 for demo
    print(f"Scraping article {i+1}: {article['headline']}")
    driver.get(article['url'])
    time.sleep(3)  # Wait for the article page to load

    article_soup = BeautifulSoup(driver.page_source, "html.parser")

    # Extract headline
    headline = article_soup.find("h1")
    headline = headline.get_text(strip=True) if headline else ""

    # Extract date (usually in <time>)
    date = ""
    time_tag = article_soup.find("time")
    if time_tag:
        date = time_tag.get("datetime") or time_tag.get_text(strip=True)

    # Extract author (look for class with 'author')
    author = ""
    author_tag = article_soup.find(lambda tag: tag.name in ["span", "a"] and "author" in (tag.get("class") or []))
    if author_tag:
        author = author_tag.get_text(strip=True)

    # Extract article content (look for <div> with 'caas-body' or fallback to <p> tags)
    content = ""
    content_div = article_soup.find("div", class_="caas-body")
    if content_div:
        content = content_div.get_text(separator="\n", strip=True)
    else:
        paragraphs = article_soup.find_all("p")
        content = "\n".join([p.get_text(strip=True) for p in paragraphs])

    detailed_articles.append({
        "headline": headline,
        "date": date,
        "author": author,
        "content": content,
        "url": article['url']
    })

    # Optional: Wait between requests to be polite
    time.sleep(2)

# Step 7: Convert the scraped data to a DataFrame for easy viewing and save to CSV if needed.

import pandas as pd
df = pd.DataFrame(detailed_articles)
df.head()  # Show the first few articles

# Optional: Save to CSV
df.to_csv("yahoo_finance_news_detailed.csv", index=False)
